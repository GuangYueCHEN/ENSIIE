My note for data analyse with Spark

一、Introduce

			-Why use Spark Clusters
				Something is happening all over the world.
				The amount of data that people are collecting,
				is increasing and it is increasing fast.
				For a while, this can be handled by
				increasing computer memory.
				One gigabyte, 10 gigabyte, 128 gigabyte.
				But at some point it becomes impossible
				to store all of the data on one computer.
				Instead, we have to use clusters with 10's, hundreds,
				or even thousands of computers.

			-Big Data analytics
				when analyzing big data, there are two latency(延迟)
				the latency of read and write operations 读写延迟
				dominates the latency of the computation. 与计算延迟
				Different types of storage offer different latency, capacity and price. 
				
				Much of big data analytics
				revolves around methods for organizing storage
				and computation in such a way
				that maximize speed while minimizing cost.
				A central concept in this context is storage locality,
				which we will talk about in the next video.

			-Cache 


			-acess locality 存储访问
				Memory is broken into pages.
				and if software uses the same page,
				or the same neighboring pages, repeatedly,
				then it has good access locality.
				Hardware is designed to speed up this type of software.

				temporal locality ： repeat access to the same memory location 
				spatial locality.

			-memory latency
				bitween CPU issuing a read or write commande and when it complete
			-SSD 
				sequential access 
二、	Spark
			-Spark Context:
				run on the	main node to contorl the other nodes
				sc=SparkContext()
			-RDD:
				resilient distributed dataset ,just like RDD
				RDD command to control, and use collect() can get it to a list, but il will lose parallelism
				so we can use just like first(5) take(5) to make sure or sample()随机抽样
			-key-value RDD:
				action: lookup():返回这个key的value list


三、snow data
			-data missing: common problem

			-RMS: 我们用RMS来量化不同因素的影响。 The RMS we used to quantify the effect of different factor.