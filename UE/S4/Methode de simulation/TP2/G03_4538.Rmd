---
title: "TP2"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
```{r,include=FALSE}
rm(list = ls())
library(ggplot2)
library(basicTrendline)
```


#Exercice 1.
###Q1
$$
\begin{aligned}
E(Y)&=E(X)=\frac{1}{6}=\sum_{Y\subset{1,0,-1}}Y*P(Y)\\
&=1*c+(-1)*(1-c-\frac{5}{6})+0*\frac{5}{6}=\frac{1}{6}\\
c&=\frac{1}{6}
\end{aligned}
$$
Donc on $P(Y=1)=\frac{1}{6}$ et $P(Y=-1)=0$

###Q2
$$
\begin{aligned}
E(Y^2)&=\sum_{Y\subset{1,0,-1}}Y^2*P(Y)\\
&=1^2*\frac{1}{6}+(-1)^2*(0)+0*\frac{5}{6}=\frac{1}{6}\\
Var(Y)&=E(Y^2)-(E(Y))^2\\
&=\frac{1}{6}-(\frac{1}{6})^2\\
&=\frac{5}{36}
\end{aligned}
$$
#pourquoi Var Y < Var X#
La probabilite que Y = -1 est nulle, on a donc moins de valeurs pour Y donc la variance est plus faible.

###Q3
```{r,warning=FALSE}
simuY<-function(){
  X=c(0,1)
  P=c(5/6,1/6)
  C<-c(P[1])
    C<-c(C,C[1]+P[2])

  k<-1
  u<-runif(1,0,1)
  while(u>C[k]){
    k<-k+1
  }
  return (X[k])
}
N=1000
echantillonY=c()
for (i in 1:1000){
  echantillonY<-c(echantillonY,simuY())
}


dat <- data.frame(echantillonY)
plot1 <- ggplot(dat,aes(x=dat$echantillonY)) + geom_histogram(fill = "darkorchid", alpha = 0.2) + xlab("Echantillon")
plot1
```

```{r,warning=FALSE}
simuX<-function(){
  X=c(-1,0,1)
  P=c(1/3,1/6,1/2)
  C<-c(P[1])
    C<-c(C,C[1]+P[2])
C<-c(C,C[2]+P[3])

  k<-1
  u<-runif(1,0,1)
  while(u>C[k]){
    k<-k+1
  }
  return (X[k])
}
N=1000
echantillonX=c()
for (i in 1:1000){
  echantillonX<-c(echantillonX,simuX())
}


dat2 <- data.frame(echantillonX)
plot2 <- ggplot(dat2,aes(x=dat2$echantillonX))+ geom_histogram(fill = "darkorchid", alpha = 0.2) + xlab("Echantillon")
plot2
```

```{r,warning=FALSE}
Z_x=c()
for(i in 1:1000){
  Z_x=c(Z_x,mean(echantillonX[1:i]))
}
Z_y=c()
for(i in 1:1000){
  Z_y=c(Z_y,mean(echantillonY[1:i]))
}


plot(Z_y,ylim = c(0,0.4),type='line',col='red',ylab="")
par(new=TRUE) 
plot(Z_x,ylim = c(0,0.4),type='line',col='blue',ylab="")
```

On remarque les deux graphes converge a 0.16, mais la deffirence est pour echatillon y , la graphe varie moins au debut.

###Q4
```{r,warning=FALSE}
alpha=0.05
c_alpha=qnorm(1-alpha/2)

Sny=0  
for(i in 1:1000){
  Sny=Sny+echantillonY[i]^2+2*echantillonY[i]*Z_y[i]+Z_y[i]^2
}
Sny=Sny/999
epsY=c_alpha*Sny/sqrt(N)
plot(Z_y,ylim = c(0,0.4),type='line',col='red',ylab="",main=" la moyenne empirique de Y avec confiance")

abline(h=1/6 +epsY)
abline(h=1/6-epsY)
```



```{r,warning=FALSE}
alpha=0.05
c_alpha=qnorm(1-alpha/2)

Snx=0  
for(i in 1:1000){
  Snx=Snx+echantillonX[i]^2+2*echantillonX[i]*Z_x[i]+Z_x[i]^2
}
Snx=Snx/999
epsX=c_alpha*Snx/sqrt(N)
plot(Z_x,ylim = c(0,0.4),type='line',col='red',ylab="",main=" la moyenne empirique de X avec confiance")

abline(h=1/6 +epsX)
abline(h=1/6-epsX)
```
Pour les deux graphes, on remarque que les valeurs sont de plus en plus entre dans l'intervalle de confiance. On trouve pour l'echantillon Y , il converge rapidement. 
###Q5
```{r,warning=FALSE}


for(i in 1000:-1:1){
  d=0
 
  if(Z_y[i]<1/6 -epsY){
   
    d=i
    break
  }else if(Z_y[i]>1/6 +epsY){
    d=i
    break
  }
  
}
N0_y<-d
print("N0 d'echantillon y :")
N0_y


for(i in 1000:-1:1){
  d=0
 
  if(Z_x[i]<1/6 -epsX){
   
    d=i
    break
  }else if(Z_x[i]>1/6 +epsX){
    d=i
    break
  }
  
}
N0_x<-d
print("N0 d'echantillon x:")
N0_x
```
On trouve que pour le valeur N0 , le N0 de y est plus petit.
#Exercice 2.
###Q1 
Donnez une estimation de $E(g(x))$
```{r}
x<-rnorm(10000)
sum<-0
for(i in 1:10000){
  if(x[i]>=3.5){
    sum<-sum+x[i]
  }
}
esti<-sum/10000
esti

```
###Q2 
Tracer sur le meme graphe  N->$\overline{Xn}$
```{r,warning=FALSE}
alpha=0.05
c_alpha=qnorm(1-alpha/2)

Z_x=c()
for(i in 1:10000){
  Z_x=c(Z_x,mean(x[1:i]))
}

Snx=0  
for(i in 1:10000){
  Snx=Snx+x[i]^2+2*x[i]*Z_x[i]+Z_x[i]^2
}
Snx=Snx/9999
epsX=c_alpha*Snx/sqrt(10000)
plot(Z_x,type='line',col='red',ylab="",main=" la moyenne empirique de X avec confiance")

abline(h=0 +epsX)
abline(h=0-epsX)

```


###Q3
(a)Identifiez la fonction $\psi$ tell que $E(g(x))=E(\psi(Z^{\mu}))$

Parce que $Z^{\mu}\sim N(\mu;1)$, $\psi(y)=(y-\mu)1_{y\ge\mu+3.5}$


(b)On pose $\mu=2.5$, pour N = 1000, puis pour N = 10000 par la méthode d’échantillonage préférentie
```{r}
p_x<-rnorm(1000,2.5,1)

sum<-0
for(i in 1:1000){
  if(p_x[i]>3.5){
  p_x[i]=exp(2.5*(2.5-2*p_x[i])/(2*1))
  
    sum<-sum+p_x[i]
  }
}
print('for N=1000:')
sum/1000

p_x<-rnorm(10000,2.5,1)


sum<-0
for(i in 1:10000){
  if(p_x[i]>3.5){
  p_x[i]=exp(2.5*(2.5-2*p_x[i])/(2*1))

    sum<-sum+p_x[i]
  }
}
print('for N=10000:')
sum/10000
```
(c)Pour N = 1, . . . , 10000, tracez sur le même graphe les fonctions N  → $\frac{g(X1 )+...g(XN )}{N}$ et
 N  → $\frac{\psi(Z1μ)+...\psi(ZNμ )}{N},$ pour μ = 2.5.

```{r,warning=FALSE}
p_x<-rnorm(10000,2.5,1)

sum<-0
for(i in 1:10000){
  if(p_x[i]>=3.5){
    p_x[i]=exp(2.5*(2.5-2*p_x[i])/(2*1))
  
    
  }else{
    p_x[i]=0
  }
}

Z_p=c()
for(i in 1:10000){
  Z_p=c(Z_p,mean(p_x[1:i]))
}

Z_x=c()
for(i in 1:10000){
  if(x[i]<3.5){
    x[i]=0
  }
  Z_x=c(Z_x,mean(x[1:i]))
}

plot(Z_p,type='line',col='blue',ylab="")


plot(Z_x,type='line',col='red',ylab="")


```
 On trouve que la simulation d'importance converge plus rapidement, elle a la plus petite variance.

(d) faire un zoom pour 1,..,1000
```{r,warning=FALSE}
p_x<-rnorm(1000,2.5,1)

sum<-0
for(i in 1:1000){
  if(p_x[i]>=3.5){
    p_x[i]=exp(2.5*(2.5-2*p_x[i])/(2*1))
  
    
  }else{
    p_x[i]=0
  }
}

Z_p=c()
for(i in 1:1000){
  Z_p=c(Z_p,mean(p_x[1:i]))
}


x<-rnorm(1000)
for(i in 1:1000){
  if(x[i]<3.5){
    x[i]=0
  }
}

Z_x=c()
for(i in 1:1000){
  Z_x=c(Z_x,mean(x[1:i]))
}





plot(Z_p,type='line',col='blue',ylab="")


plot(Z_x,type='line',col='red',ylab="")
```
La simulation pour un N élevé converge plus rapidement car sa variance est plus faible.

(e)PourN=1,...,10000,tracez sur une nouvelle fenêtre graphique la fonction N -> ψ(Z1μ)+...ψ(ZNμ) /N
```{r,warning=FALSE}

p_x<-rnorm(10000,2.5,1)
sum<-0
for(i in 1:10000){
  mu<-6*runif(1,0,1)
  if(p_x[i]>=3.5){
  p_x[i]=exp(mu*(mu-2*p_x[i])/(2*1))
  }else{
    p_x[i]=0
  }

}

Z_p=c()
for(i in 1:10000){
  Z_p=c(Z_p,mean(p_x[1:i]))
}

Snx=0  
for(i in 1:10000){
  Snx=Snx+p_x[i]^2+2*p_x[i]*Z_p[i]+Z_p[i]^2
}
Snx=Snx/9999
epsX=c_alpha*Snx/sqrt(10000)
plot(Z_p,type='line',col='blue',ylab="")

abline(h=Z_p[10000] +epsX,col='red')
abline(h=Z_p[10000] -epsX,col='red')
```
Ici on trouve que, a la fin, le résultat n'est pas dans l'intervalle de confiance avant un indice élevé, c'est peut-être car les points où (x>3.5) sont trop peu fréquents. Il termine toujours cependant par être dans l'intervalle de confiance.

#4 On veut chercher le paramètre μ∗ qui minimise Eψ2(Zμ).
(a)Spécifiez la fonction K qui vérifie Eψ2(Zμ) = E(K(μ, ξ)), avec ξ ∼ N (0, 1)
$$
\begin{aligned}
E(\psi^2(Z^{\mu}))&=E(\psi^2(\mu+\sigma\xi))) \\
&= E(g^2(\mu+\sigma\xi)exp(\frac{2\mu(\mu-2(\mu+\sigma\xi))}{2\sigma^2}))\\
&= \mathbb{E}K(\mu,\xi),\quad\xi\backsim N(0,1)\\
K(\mu,\xi)&=g^2(\mu+\sigma\xi)exp(\frac{2\mu(\mu-2(\mu+\sigma\xi))}{2\sigma^2})\\
&=g^2(\mu+\sigma\xi)exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})\\
\end{aligned}
$$
(b) Utilisez l’algorithme suivant pour donner une approximation de μ∗:
$$
\begin{aligned}
K(\mu,\xi)&=g^2(\mu+\sigma\xi)exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})\\
K'(\mu,\xi)&=2*g(\mu+\sigma\xi)*g'(\mu+\sigma\xi)exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})+\\
 &  g^2(\mu+\sigma\xi)exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})*(\frac{-4\mu-4\sigma\xi}{2\sigma^2})\\
 K'(\mu,\xi)&=0 \quad if \quad \mu+\sigma\xi<3.5\\
  K'(\mu,\xi)&=2*(\mu+\sigma\xi)exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})+(\mu+\sigma\xi)^2exp(\frac{-2\mu^2-4\mu\sigma\xi}{2\sigma^2})*(\frac{-4\mu-4\sigma\xi}{2\sigma^2}) \quad if \quad \mu+\sigma\xi \ge 3.5
\end{aligned}
$$

```{r}
K<-function(mu,delta,xi){
  
  if((mu+delta*xi)>=3.5){
  K=2*(mu+delta*xi)*exp((-2*mu^2-4*mu*delta*xi)/(2*delta^2)) +(mu+delta*xi)^2*exp((-2*mu^2-4*mu*delta*xi)/(2*delta^2))*(-4*mu-4*mu*delta*xi)/(2*delta^2)
  }else{
    K=0
  }
  return(K)
  
}


mu_esti<-runif(1,0,1)*6
for(i in 1:10000){
  xi=rnorm(1)
  
  
  mu_esti=mu_esti-(1/(i))*K(mu,1,xi)
  
}

mu_esti

```

(c) Pour N = 1, . . . , 10000, tracez la fonction N,Comparez avec les résultats obtenus pour μ = 0 et μ = 2.5.

```{r,warning=FALSE}
p_x3<-rnorm(10000,mu_esti,1)
sum<-0
for(i in 1:10000){
  mu<-mu_esti
  if(p_x3[i]>=3.5){
  p_x3[i]=exp(mu*(mu-2*p_x3[i])/(2*1))
  }else{
    p_x3[i]=0
  }

}

Z_p3=c()
for(i in 1:10000){
  Z_p3=c(Z_p3,mean(p_x3[1:i]))
}
plot(Z_p3,type='line',col='green',ylab="",main="avec mu =la résultat obtenu ")
```
On constate que la convergence est plus perturbée avec mu = 2,5, sans doute parce qu'avec un mu = 2,5, on aura plus de valeurs supérieures à 3,5. Il y a donc une différence de convergence (ainsi que d'asymptote) pour mu = 0 et mu = 2,5.


#5 Pour chacun des cas μ = 0, μ = 2.5, μ = μ∗, déterminer numériquement l’entier approximatif N0 à partir duquel l’erreur d’estimation est d’ordre 10−2 (au niveau de confiance 95%). On fera croître N par pas de 10 pour atteindre le critère d’erreur d’approximation (voir cours).
```{r,warning=FALSE}

p_x<-rnorm(10000,2.5,1)
sum<-0
for(i in 1:10000){
  mu<-2.5
  if(p_x[i]>=3.5){
  p_x[i]=exp(mu*(mu-2*p_x[i])/(2*1))
  }else{
    p_x[i]=0
  }

}

Z_p=c()
for(i in 1:10000){
  Z_p=c(Z_p,mean(p_x[1:i]))
}


p_x2<-rnorm(10000,0,1)
sum<-0
for(i in 1:10000){
  mu<-0
  if(p_x2[i]>=3.5){
  p_x2[i]=exp(mu*(mu-2*p_x2[i])/(2*1))
  }else{
    p_x2[i]=0
  }

}

Z_p2=c()
for(i in 1:10000){
  Z_p2=c(Z_p2,mean(p_x2[1:i]))
}

p_x3<-rnorm(10000,mu_esti,1)
sum<-0
for(i in 1:10000){
  mu<-mu_esti
  if(p_x3[i]>=3.5){
  p_x3[i]=exp(mu*(mu-2*p_x3[i])/(2*1))
  }else{
    p_x3[i]=0
  }

}

Z_p3=c()
for(i in 1:10000){
  Z_p3=c(Z_p3,mean(p_x3[1:i]))
}









plot(Z_p,type='line',col='blue',ylab="",main="avec mu =2.5")
Snx=0  
for(i in 1:10000){
  Snx=Snx+p_x[i]^2+2*p_x[i]*Z_p[i]+Z_p[i]^2
}
Snx=Snx/9999
epsX=c_alpha*Snx/sqrt(10000)

abline(h=Z_p[10000] +epsX,col="red")
abline(h=Z_p[10000]-epsX,col="red")

plot(Z_p2,type='line',col='red',ylab="",main="avec mu =0")
Snx2=0  
for(i in 1:10000){
  Snx2=Snx2+p_x2[i]^2+2*p_x2[i]*Z_p2[i]+Z_p2[i]^2
}
Snx=Snx/9999
epsX2=c_alpha*Snx/sqrt(10000)

abline(h=Z_p2[10000] +epsX2,col="blue")
abline(h=Z_p2[10000]-epsX2,col="blue")

plot(Z_p3,type='line',col='green',ylab="",main="avec mu =la résultat obtenu ")
Snx3=0  
for(i in 1:10000){
  Snx3=Snx3+p_x3[i]^2+2*p_x3[i]*Z_p3[i]+Z_p3[i]^2
}
Snx=Snx/9999
epsX3=c_alpha*Snx/sqrt(10000)

abline(h=Z_p3[10000] +epsX3,col="red")
abline(h=Z_p3[10000]-epsX3,col="red")

```









