---
title: "TP2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Guangyue CHEN

Jiahui XU



###Application THE Boston housing data set
#(a)upload the data
```{r include=TRUE}
rm(list=ls())
library(mlbench)
data(BostonHousing)
```

the first step, we try to use linear regression.
```{r include=TRUE}
modreg<-lm(medv~.,BostonHousing)
summary(modreg)

```
This linear model has the residual stadard error which is 4.745. But with the high R-squared and the small p-value of F-test, we don't refuse this mod. So we use the different ways to select our linear model:
```{r include=FALSE}
regbackward = step(modreg, direction = 'backward')
regforward = step(lm(medv~1, data = BostonHousing), list(upper = modreg), direction = 'forward')
regbic = step(modreg, direction = 'both', k = log(nrow(BostonHousing)))
regboth = step(modreg, direction = 'both')

```
Their aics are the same, we can choose no matter which one.
```{r include=TRUE}
AIC(regforward)
AIC(regbackward)
AIC(regbic)
AIC(regboth)
```

```{r include=TRUE}
reg = lm(formula(regbackward), data = BostonHousing)
summary(reg)
Y_esti<-predict(reg,BostonHousing)
Y<-BostonHousing$medv
Non_biased_residual<-function(Y,Y_esti,p){
sum=0

for(i in seq(1,length(Y))){
  sum<-sum+(Y_esti[i]-Y[i])^2

}
NBR<- sqrt(sum/(length(Y)-p+1))

return(NBR)
}
Non_biased_residual(Y,Y_esti,13)
```
So that we obtain the model after the selection, with the function "predict" we can gain the estimation.

#LASSO

The next step, we try the Lasso regression:
```{r include=TRUE}
library(lars)
Y<-as.matrix(BostonHousing$medv)
X<-apply(as.matrix(subset(BostonHousing,select=-medv)),2,as.numeric)
modlasso=lars(x=X,y=Y,type="lasso")
plot(modlasso)
plot(modlasso$lambda)

```
From these two graphs, we can see the evolution of the values of the coefficients for different values of the penalized coefficient. And after the beta bigger than 13, the coefficients become more stable.
```{r include=TRUE}
modlasso$lambda[which.min(modlasso$RSS)-1]
```

With the help of criteria RSS, we choose the 16th lambda which is 0.0996448. And we found that the residual standard error is less than the Previous method but the difference is small.

```{r include=TRUE}
coef<-predict.lars(modlasso,X,type="coefficient",mode="lambda",s=0.0996448)
coef$coefficients
Y_esti<-predict.lars(modlasso,X,type="fit",mode="lambda",s=0.0996448)
Y_esti<-Y_esti$fit
#data.frame(Y_esti,Y)
print("residual standard error")
Non_biased_residual(Y,Y_esti,13)
```

#RIDGE
```{r include=TRUE}
library(MASS)
modridge<-lm.ridge(medv~.,data=BostonHousing,lambda=seq(0,10,0.01))
plot(modridge)
lambda<-modridge$lambda[which.min(modridge$GCV)]

abline(v=lambda)

plot(x=seq(0,10,0.01),modridge$GCV,xlab = "lambda")
abline(v=lambda)

```

For the ridge regression, with the smallest GCV, wo choose the lambda which is 4.26. So we can use the regression model whose lambda equals 4.26.
```{r include=TRUE}
modridge<-lm.ridge(medv~.,data=BostonHousing,lambda=lambda)

coef<-coef(modridge)
coef
un<-matrix(1,nrow=length(Y),ncol=1)
Y_esti<-cbind(un,X)%*%as.vector(coef)
Non_biased_residual(Y,Y_esti,13)
```
So we obtain the result.

What's more, I think about how about it with the new data.
```{r, include=TRUE}

smp1<-sample(nrow(BostonHousing), nrow(BostonHousing)*0.75)
train_data=BostonHousing[smp1,]
test_data=BostonHousing[-smp1,]

```


With linear regression
```{r, include=TRUE}
modreg<-lm(medv~.,train_data)
regbackward = step(modreg, direction = 'backward')
reg = lm(formula(regbackward), data = train_data)

```
without the selection of various:
```{r, include=TRUE}
Y_esti<-predict(modreg,newdata=test_data)
Y_test<-test_data$medv
Non_biased_residual(Y_test,Y_esti,13)
```
The linear regression backward:
```{r, include=TRUE}
Y_esti<-predict(reg,newdata=test_data)
Y_test<-test_data$medv
Non_biased_residual(Y_test,Y_esti,13)
```
#LASSO

```{r include=TRUE}
Y<-as.matrix(train_data$medv)
X<-apply(as.matrix(subset(train_data,select=-medv)),2,as.numeric)
modlasso=lars(x=X,y=Y,type="lasso")
X_test<-apply(as.matrix(subset(test_data,select=-medv)),2,as.numeric)
Y_esti<-predict.lars(modlasso,X_test,type="fit",mode="lambda",s=modlasso$lambda[which.min(modlasso$RSS)-1])
Y_esti<-Y_esti$fit
Y_test<-test_data$medv
Non_biased_residual(Y_test,Y_esti,13)
```

#Ridge
```{r include=TRUE}

modridge<-lm.ridge(medv~.,data=train_data,lambda=seq(0,10,0.01))
lambda<-modridge$lambda[which.min(modridge$GCV)]

```

For the ridge regression, with the smallest GCV, wo choose the lambda which is 4.26. So we can use the regression model whose lambda equals 4.26.
```{r include=TRUE}
modridge<-lm.ridge(medv~.,data=train_data,lambda=lambda)
X_test<-apply(as.matrix(subset(test_data,select=-medv)),2,as.numeric)
coef<-coef(modridge)
Y_test<-test_data$medv
un<-matrix(1,nrow=length(Y_test),ncol=1)
Y_esti<-cbind(un,X_test)%*%as.vector(coef)
Non_biased_residual(Y_test,Y_esti,13)
```
That's all. I find that for these data, the linear regression backward and the lasso regression is better than Ridge regression. And the normal linear regression fit the new data worse.





